{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOv/iqSEvjwlRyHZPP2DbSz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0B1eR4MYfF3i","executionInfo":{"status":"ok","timestamp":1742434780538,"user_tz":360,"elapsed":1376,"user":{"displayName":"Andrew Pownuk","userId":"09849958492689926656"}},"outputId":"6d1c9ab2-cf13-4471-fc11-142ae2855c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Emissions dataset successfully loaded from GitHub.\n","Emissions Data Columns: ['Plant', 'Source', 'Parameter', 'Units', 'TimeStamp', 'Value', 'Description']\n","Emissions data successfully pivoted.\n","Weather dataset successfully loaded from GitHub.\n","Weather data successfully merged.\n","Number of rows after merging: 2269\n","Columns in merged dataset: ['date', 'Source', 'COTONS', 'GFLOW_BA', 'GFLOW_BT', 'HEATINBA', 'HEATINBT', 'HEAT_QA', 'LOADMWBA', 'LOADMWBT', 'NH3TONS', 'NOXTONS', 'SO2TONS', 'UNITONBA', 'UNITONBT', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun']\n","Dataset before cleaning saved to: /content/drive/My Drive/006-Dataset-For-Predictions.csv\n","Column 'wpgt' removed from dataset.\n","Column 'tsun' removed from dataset.\n","Cleaned dataset saved to: /content/drive/My Drive/007-Dataset-For-Predictions-Cleaned.csv\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","\n","# Mount Google Drive (if not already mounted)\n","drive.mount('/content/drive')\n","\n","# Define file paths\n","github_emissions_url = \"https://raw.githubusercontent.com/apownukepcc/spring-2025-datathon/main/004-Emissions-Data.csv\"\n","github_weather_url = \"https://raw.githubusercontent.com/apownukepcc/spring-2025-datathon/main/001-Weather.csv\"\n","output_file = \"/content/drive/My Drive/006-Dataset-For-Predictions.csv\"  # Before cleaning\n","cleaned_output_file = \"/content/drive/My Drive/007-Dataset-For-Predictions-Cleaned.csv\"  # After cleaning\n","\n","# Load the emissions dataset\n","try:\n","    emissions_data = pd.read_csv(github_emissions_url)\n","    print(\"Emissions dataset successfully loaded from GitHub.\")\n","except Exception as e:\n","    print(f\"Error loading emissions dataset: {e}\")\n","    exit()\n","\n","# Print columns for verification\n","print(\"Emissions Data Columns:\", emissions_data.columns.tolist())\n","\n","# Convert 'TimeStamp' to date-only format\n","if \"TimeStamp\" not in emissions_data.columns:\n","    print(\"Error: 'TimeStamp' column not found in emissions data.\")\n","    exit()\n","else:\n","    emissions_data[\"date\"] = pd.to_datetime(emissions_data[\"TimeStamp\"]).dt.date\n","\n","# Pivot the emissions dataset\n","try:\n","    final_data = emissions_data.pivot_table(index=[\"date\", \"Source\"],\n","                                            columns=\"Parameter\",\n","                                            values=\"Value\").reset_index()\n","    print(\"Emissions data successfully pivoted.\")\n","except Exception as e:\n","    print(f\"Error during pivoting the data: {e}\")\n","    exit()\n","\n","# Load the weather dataset\n","try:\n","    weather_data = pd.read_csv(github_weather_url)\n","    print(\"Weather dataset successfully loaded from GitHub.\")\n","\n","    # Convert weather dataset 'date' column to match emissions dataset\n","    if \"date\" not in weather_data.columns and \"Date\" in weather_data.columns:\n","        weather_data.rename(columns={\"Date\": \"date\"}, inplace=True)\n","\n","    if \"date\" in weather_data.columns:\n","        weather_data[\"date\"] = pd.to_datetime(weather_data[\"date\"]).dt.date\n","        # Drop rows where 'date' is NaN\n","        weather_data = weather_data.dropna(subset=[\"date\"])\n","    else:\n","        print(\"Error: 'date' column not found in weather data. Skipping merge.\")\n","        weather_data = None\n","except Exception as e:\n","    print(f\"Error loading weather dataset: {e}\")\n","    weather_data = None\n","\n","# Merge only if weather data is available\n","if weather_data is not None:\n","    try:\n","        final_data_with_weather = pd.merge(final_data, weather_data, on=\"date\", how=\"left\")\n","        print(\"Weather data successfully merged.\")\n","        print(f\"Number of rows after merging: {len(final_data_with_weather)}\")\n","    except Exception as e:\n","        print(f\"Error merging weather data: {e}\")\n","else:\n","    final_data_with_weather = final_data  # Use emissions dataset without merging\n","\n","# **Re-check column names after merging**\n","print(\"Columns in merged dataset:\", final_data_with_weather.columns.tolist())\n","\n","# Save dataset BEFORE cleaning\n","try:\n","    final_data_with_weather.to_csv(output_file, index=False)\n","    print(f\"Dataset before cleaning saved to: {output_file}\")\n","except Exception as e:\n","    print(f\"Error saving dataset before cleaning: {e}\")\n","\n","# **Step 1: Remove Unwanted Columns (`wpgt`, `tsun`)**\n","columns_to_remove = [\"wpgt\", \"tsun\"]\n","existing_columns = final_data_with_weather.columns.tolist()\n","\n","# Remove columns only if they exist in the dataset\n","for col in columns_to_remove:\n","    if col in existing_columns:\n","        final_data_with_weather.drop(columns=[col], inplace=True)\n","        print(f\"Column '{col}' removed from dataset.\")\n","\n","# **Step 2: Drop Rows with Missing Values**\n","df_cleaned = final_data_with_weather.dropna()\n","\n","# Check the number of rows remaining after cleaning\n","num_rows_after_cleaning = len(df_cleaned)\n","\n","# Save the cleaned dataset **only if it is not empty**\n","if num_rows_after_cleaning > 0:\n","    try:\n","        df_cleaned.to_csv(cleaned_output_file, index=False)\n","        print(f\"Cleaned dataset saved to: {cleaned_output_file}\")\n","    except Exception as e:\n","        print(f\"Error saving cleaned dataset: {e}\")\n","else:\n","    print(\"Warning: The dataset is empty after cleaning!\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"dMGmTOj_g1hU"},"execution_count":null,"outputs":[]}]}