{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJ/4wOr9yvOe92gGAA2UhJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0Q7OUL1z84l","executionInfo":{"status":"ok","timestamp":1742445798833,"user_tz":360,"elapsed":6371,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"80760b45-24d6-4f5c-f906-95084c4143ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000038\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000003\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000305\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000027\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000803\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004322 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000133\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000298\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 912\n","[LightGBM] [Info] Number of data points in the train set: 1696, number of used features: 7\n","[LightGBM] [Info] Start training from score 0.000097\n","Prediction Results with Additional Accuracy Metrics:\n","              Parameter           MSE      RMSE  R2 Score       MAE  \\\n","0  SO2TONS_per_LOADMWBA  3.475354e-10  0.000019 -0.350354  0.000014   \n","1  SO2TONS_per_LOADMWBT  7.976727e-11  0.000009 -0.772339  0.000001   \n","2  NH3TONS_per_LOADMWBA  1.700940e-07  0.000412 -0.015605  0.000168   \n","3  NH3TONS_per_LOADMWBT  1.466640e-07  0.000383 -0.011741  0.000033   \n","4  NOXTONS_per_LOADMWBA  2.164107e-06  0.001471 -3.683081  0.000381   \n","5  NOXTONS_per_LOADMWBT  2.089575e-06  0.001446 -3.755967  0.000196   \n","6   COTONS_per_LOADMWBA  1.611318e-06  0.001269 -2.884745  0.000263   \n","7   COTONS_per_LOADMWBT  1.586905e-06  0.001260 -2.867856  0.000197   \n","\n","      Median AE  Explained Variance  \n","0  1.047085e-05           -0.340168  \n","1  1.186857e-07           -0.772297  \n","2  1.275003e-04           -0.012403  \n","3  8.185834e-06           -0.010231  \n","4  2.081018e-04           -3.680984  \n","5  1.665453e-05           -3.748298  \n","6  1.012019e-04           -2.880673  \n","7  2.408654e-05           -2.861852  \n","\n","Comparison for specific test date (2022-07-15):\n","                        Actual  Predicted\n","SO2TONS_per_LOADMWBA  0.000031   0.000038\n","SO2TONS_per_LOADMWBT  0.000003   0.000003\n","NH3TONS_per_LOADMWBA  0.000178   0.000331\n","NH3TONS_per_LOADMWBT  0.000015   0.000022\n","NOXTONS_per_LOADMWBA  0.000613   0.000685\n","NOXTONS_per_LOADMWBT  0.000051   0.000062\n","COTONS_per_LOADMWBA   0.000104   0.000163\n","COTONS_per_LOADMWBT   0.000009   0.000013\n","\n","--- ChatGPT Comment on Accuracy ---\n","\n","The accuracy of the predictions varies across the different target metrics. For example, the model performs relatively well for SO2 and CO emissions, with low RMSE and MAE values. However, for NH3 and NOX emissions, the model shows higher errors and lower R² scores, indicating room for improvement.\n","\n","To enhance the predictive performance, several recommendations can be considered:\n","1. Feature Engineering: Explore additional relevant features that could potentially improve the model's predictive power, such as historical emissions data, industry activity levels, or geographical factors.\n","2. Hyperparameter Tuning: Experiment with different hyperparameters for the LightGBM Regressor to optimize the model's performance.\n","3. Data Preprocessing: Ensure data quality by addressing missing values, outliers, and scaling the features appropriately to enhance the model's robustness.\n","4. Model Evaluation: Consider using cross-validation techniques to assess the model's generalization ability and stability across different subsets of the data.\n","5. Ensemble Methods: Explore ensemble techniques, such as stacking or blending multiple models, to potentially boost predictive accuracy.\n","\n","Overall, while the current model shows promising results for some metrics, there is still room for improvement, and implementing the above recommendations could help enhance the accuracy of the predictions.\n"]}],"source":["# ----------------------------------\n","# 1. Mount Google Drive and Imports\n","# ----------------------------------\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import asyncio\n","import nest_asyncio\n","import openai\n","\n","import numpy as np\n","import pandas as pd\n","\n","# Import LGBMRegressor\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    mean_squared_error,\n","    r2_score,\n","    mean_absolute_error,\n","    median_absolute_error,\n","    explained_variance_score\n",")\n","\n","nest_asyncio.apply()  # Allows asyncio.run() in notebooks\n","\n","# ----------------------------------\n","# 2. Configure OpenAI Client\n","# ----------------------------------\n","\n","# Read your API key from Google Drive\n","with open('/content/drive/MyDrive/key.txt', 'r') as file:\n","    api_key = file.read().strip()\n","\n","client = openai.AsyncOpenAI(api_key=api_key)\n","\n","# ----------------------------------\n","# 3. Load and Preprocess the Dataset\n","# ----------------------------------\n","\n","url = \"https://raw.githubusercontent.com/apownukepcc/spring-2025-datathon/main/009-Dataset-For-Predictions-With-Specific-Emissions.csv\"\n","df = pd.read_csv(url)\n","\n","# Convert 'date' column to datetime\n","df['date'] = pd.to_datetime(df['date'])\n","\n","# Define the specific date to be included in the test set later\n","specific_test_date = pd.to_datetime(\"2022-07-15\")\n","\n","# Ensure the specific test date is in the dataset\n","if specific_test_date not in df['date'].values:\n","    raise ValueError(f\"No data available for the date: {specific_test_date}\")\n","\n","# Extract the row for the specific test date and remove it from the main dataset\n","specific_test_row = df[df['date'] == specific_test_date]\n","df = df[df['date'] != specific_test_date]\n","\n","# Define feature columns (weather parameters)\n","feature_cols = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres']\n","\n","# Define target columns (specific emissions metrics)\n","target_cols = [\n","    'SO2TONS_per_LOADMWBA', 'SO2TONS_per_LOADMWBT',\n","    'NH3TONS_per_LOADMWBA', 'NH3TONS_per_LOADMWBT',\n","    'NOXTONS_per_LOADMWBA', 'NOXTONS_per_LOADMWBT',\n","    'COTONS_per_LOADMWBA', 'COTONS_per_LOADMWBT'\n","]\n","\n","# Ensure no missing values in features and targets\n","df = df.dropna(subset=feature_cols + target_cols)\n","\n","# ----------------------------------\n","# 4. Train-Test Split\n","# ----------------------------------\n","\n","X = df[feature_cols]\n","y = df[target_cols]\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Append the specific test row to the test set (it will be the last row)\n","X_test = pd.concat([X_test, specific_test_row[feature_cols]])\n","y_test = pd.concat([y_test, specific_test_row[target_cols]])\n","\n","# ----------------------------------\n","# 5. Multi-Output LightGBM\n","# ----------------------------------\n","# Wrap LGBMRegressor with MultiOutputRegressor to handle multiple targets\n","\n","base_lgbm = LGBMRegressor(\n","    n_estimators=100,      # Number of boosting rounds\n","    learning_rate=0.1,     # Learning rate\n","    random_state=42\n",")\n","\n","multi_lgbm = MultiOutputRegressor(base_lgbm)\n","multi_lgbm.fit(X_train, y_train)\n","\n","# Predict on the test set\n","predictions = multi_lgbm.predict(X_test)\n","\n","# ----------------------------------\n","# 6. Compute Accuracy Metrics\n","# ----------------------------------\n","\n","mse = mean_squared_error(y_test, predictions, multioutput='raw_values')\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, predictions, multioutput='raw_values')\n","mae = mean_absolute_error(y_test, predictions, multioutput='raw_values')\n","med_ae = median_absolute_error(y_test, predictions, multioutput='raw_values')\n","explained_var = explained_variance_score(y_test, predictions, multioutput='raw_values')\n","\n","results = pd.DataFrame({\n","    'Parameter': target_cols,\n","    'MSE': mse,\n","    'RMSE': rmse,\n","    'R2 Score': r2,\n","    'MAE': mae,\n","    'Median AE': med_ae,\n","    'Explained Variance': explained_var\n","})\n","\n","print(\"Prediction Results with Additional Accuracy Metrics:\")\n","print(results)\n","\n","# Compare predictions with actual values for the specific test date\n","pred_df = pd.DataFrame(predictions, index=y_test.index, columns=target_cols)\n","specific_pred = pred_df.iloc[-1]\n","specific_actual = y_test.iloc[-1]\n","\n","comparison_df = pd.DataFrame({\n","    'Actual': specific_actual,\n","    'Predicted': specific_pred\n","})\n","\n","print(f\"\\nComparison for specific test date ({specific_test_date.date()}):\")\n","print(comparison_df)\n","\n","# ----------------------------------\n","# 7. Prepare Summary for ChatGPT\n","# ----------------------------------\n","\n","summary = f\"\"\"\n","Prediction Results with Additional Accuracy Metrics:\n","{results.to_string(index=False)}\n","\n","Comparison for specific test date ({specific_test_date.date()}):\n","{comparison_df.to_string()}\n","\n","Predictive Method Description:\n","A LightGBM Regressor (n_estimators=100, learning_rate=0.1) was wrapped with\n","MultiOutputRegressor to handle 8 target metrics (SO2, NH3, NOX, CO per LOADMWBA/BT).\n","Trained on 80% of the dataset using weather features (tavg, tmin, tmax, prcp, snow, wdir, wspd, pres).\n","The specific date (2022-07-15) was excluded during training, then appended to the test set for a focused prediction comparison.\n","Performance was evaluated using multiple metrics: MSE, RMSE, R², MAE, Median AE, and Explained Variance.\n","\n","Question:\n","Based on the above results and methodology, please provide a comment on the accuracy of these predictions\n","and any recommendations for improvement.\n","\"\"\"\n","\n","# ----------------------------------\n","# 8. Async Function to Query ChatGPT\n","# ----------------------------------\n","async def get_chatgpt_comment(user_message):\n","    response = await client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": user_message}],\n","        temperature=0.2\n","    )\n","    return response.choices[0].message.content.strip()\n","\n","# ----------------------------------\n","# 9. Send Query and Print Response\n","# ----------------------------------\n","comment_on_accuracy = asyncio.run(get_chatgpt_comment(summary))\n","print(\"\\n--- ChatGPT Comment on Accuracy ---\\n\")\n","print(comment_on_accuracy)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"3gIEs0o50DTL"},"execution_count":null,"outputs":[]}]}