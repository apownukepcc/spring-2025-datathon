{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM56p/hTEbXqGVbp2IKUWum"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMdw0s0LXDAc","executionInfo":{"status":"ok","timestamp":1741196378147,"user_tz":420,"elapsed":420632,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"f5b762c8-ba01-4847-cb38-172b20e5aeee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters for SO2TONS at LAKE-1: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n","Model for SO2TONS at LAKE-1:\n","  RMSE: 1.0449e-05\n","  R²: 4.5652e-01\n","Best parameters for SO2TONS at LAKE-2: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n","Model for SO2TONS at LAKE-2:\n","  RMSE: 1.2846e-05\n","  R²: 1.4932e-02\n","Best parameters for SO2TONS at LAKE-3: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n","Model for SO2TONS at LAKE-3:\n","  RMSE: 1.0626e-05\n","  R²: 3.3513e-01\n","Best parameters for SO2TONS at LAKE-4: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n","Model for SO2TONS at LAKE-4:\n","  RMSE: 1.2462e-05\n","  R²: 2.6677e-01\n","Best parameters for NOXTONS at LAKE-1: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n","Model for NOXTONS at LAKE-1:\n","  RMSE: 2.2924e-04\n","  R²: 2.1298e-01\n","Best parameters for NOXTONS at LAKE-2: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n","Model for NOXTONS at LAKE-2:\n","  RMSE: 2.5178e-04\n","  R²: -2.9540e-01\n","Best parameters for NOXTONS at LAKE-3: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n","Model for NOXTONS at LAKE-3:\n","  RMSE: 1.8913e-04\n","  R²: 3.1740e-01\n","Best parameters for NOXTONS at LAKE-4: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n","Model for NOXTONS at LAKE-4:\n","  RMSE: 1.9671e-04\n","  R²: 1.9992e-01\n","Best parameters for COTONS at LAKE-1: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50}\n","Model for COTONS at LAKE-1:\n","  RMSE: 8.9097e-05\n","  R²: 1.7415e-01\n","Best parameters for COTONS at LAKE-2: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n","Model for COTONS at LAKE-2:\n","  RMSE: 1.0198e-04\n","  R²: -3.3737e-01\n","Best parameters for COTONS at LAKE-3: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50}\n","Model for COTONS at LAKE-3:\n","  RMSE: 7.2013e-05\n","  R²: 2.2357e-01\n","Best parameters for COTONS at LAKE-4: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n","Model for COTONS at LAKE-4:\n","  RMSE: 9.5356e-05\n","  R²: 1.6960e-01\n","\n","Final Predictions:\n","SO2TONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 4.0632e-05\n","  Predicted Emissions_Load: 4.0254e-05\n","\n","SO2TONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 3.7988e-05\n","  Predicted Emissions_Load: 3.4406e-05\n","\n","SO2TONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 2.5462e-05\n","  Predicted Emissions_Load: 4.2177e-05\n","\n","SO2TONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 3.1469e-05\n","  Predicted Emissions_Load: 3.9735e-05\n","\n","NOXTONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 7.6493e-04\n","  Predicted Emissions_Load: 7.9548e-04\n","\n","NOXTONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 7.6357e-04\n","  Predicted Emissions_Load: 7.4970e-04\n","\n","NOXTONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 5.1540e-04\n","  Predicted Emissions_Load: 8.1152e-04\n","\n","NOXTONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 6.1329e-04\n","  Predicted Emissions_Load: 7.6843e-04\n","\n","COTONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 1.4789e-04\n","  Predicted Emissions_Load: 1.3650e-04\n","\n","COTONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 1.8202e-04\n","  Predicted Emissions_Load: 1.7225e-04\n","\n","COTONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 1.0386e-04\n","  Predicted Emissions_Load: 1.7291e-04\n","\n","COTONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 1.0353e-04\n","  Predicted Emissions_Load: 2.3309e-04\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# URLs for datasets\n","datasets = {\n","    \"SO2TONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/SO2TONS_dataset.csv\",\n","    \"NOXTONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/NOXTONS_dataset.csv\",\n","    \"COTONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/COTONS_dataset.csv\"\n","}\n","\n","# Define the peak season months (May through August)\n","peak_season_months = [5, 6, 7, 8]\n","\n","# Define lakes (sources)\n","sources = [\"LAKE-1\", \"LAKE-2\", \"LAKE-3\", \"LAKE-4\"]\n","\n","# Define the specific day for prediction\n","specific_date = pd.Timestamp(\"2022-07-15\")\n","\n","# Initialize dictionaries to store models and predictions\n","models = {}\n","predictions = {}\n","\n","# Define the parameter grid for RandomForestRegressor\n","param_grid = {\n","    'n_estimators': [50, 100, 200],         # Number of trees in the forest\n","    'max_depth': [None, 5, 10, 20],           # Maximum depth of the tree\n","    'min_samples_split': [2, 5, 10]           # Minimum samples required to split an internal node\n","}\n","\n","# Loop through each dataset (SO2TONS, NOXTONS, COTONS)\n","for parameter, url in datasets.items():\n","    # Load the dataset\n","    data = pd.read_csv(url)\n","\n","    # Convert the 'date' column to datetime\n","    data['date'] = pd.to_datetime(data['date'])\n","\n","    # Filter for peak season\n","    data = data[data['date'].dt.month.isin(peak_season_months)]\n","\n","    # Separate data by source\n","    for source in sources:\n","        source_data = data[data['Source'] == source]\n","\n","        # Check if the source data has enough rows\n","        if source_data.empty or len(source_data) < 10:\n","            print(f\"Not enough data for {parameter} at {source}. Skipping...\")\n","            continue\n","\n","        # Define predictors (weather features) and target variable\n","        predictors = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres']\n","        target = 'Emissions_Load'\n","\n","        # Drop rows with missing values\n","        source_data = source_data.dropna(subset=predictors + [target])\n","\n","        # Split the data into features (X) and target (y)\n","        X = source_data[predictors]\n","        y = source_data[target]\n","\n","        # Standardize features\n","        scaler = StandardScaler()\n","        X_scaled = scaler.fit_transform(X)\n","\n","        # Split into train and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","        # Set up the RandomForestRegressor and GridSearchCV for hyperparameter tuning\n","        rf = RandomForestRegressor(random_state=42)\n","        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","\n","        # Retrieve the best estimator\n","        best_model = grid_search.best_estimator_\n","\n","        # Evaluate the tuned model on the test set\n","        y_pred = best_model.predict(X_test)\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        r2 = r2_score(y_test, y_pred)\n","\n","        print(f\"Best parameters for {parameter} at {source}: {grid_search.best_params_}\")\n","        print(f\"Model for {parameter} at {source}:\")\n","        print(f\"  RMSE: {rmse:.4e}\")\n","        print(f\"  R²: {r2:.4e}\")\n","\n","        # Save the best model and scaler\n","        models[(parameter, source)] = (best_model, scaler)\n","\n","        # Check if the specific date exists in the source data\n","        day_data = source_data[source_data['date'] == specific_date]\n","        if not day_data.empty:\n","            # Extract feature values for the specific day and scale them\n","            specific_features = scaler.transform(day_data[predictors])\n","            specific_actual = day_data[target].iloc[0]\n","\n","            # Predict emissions/load for the specific day using the tuned model\n","            specific_prediction = best_model.predict(specific_features)[0]\n","\n","            # Save the prediction and actual value for verification\n","            predictions[(parameter, source)] = {\n","                \"features\": day_data[predictors].iloc[0],\n","                \"actual\": specific_actual,\n","                \"predicted\": specific_prediction\n","            }\n","\n","# Display all predictions at the end\n","print(\"\\nFinal Predictions:\")\n","for key, value in predictions.items():\n","    parameter, source = key\n","    print(f\"{parameter} at {source}:\")\n","    print(f\"  Features: {value['features'].to_dict()}\")\n","    print(f\"  Actual Emissions_Load: {value['actual']:.4e}\")\n","    print(f\"  Predicted Emissions_Load: {value['predicted']:.4e}\")\n","    print()\n"]}]}